{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Approach 1: Stack 3 layers of LSTM - consumed \n",
        "95% of the time and efforts, outputs were\n",
        "not satisfactory - models only as good as the data**"
      ],
      "metadata": {
        "id": "RmoxeNHjyG7b"
      },
      "id": "RmoxeNHjyG7b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approach 2: Transformer based models - simpler, faster, reliable**"
      ],
      "metadata": {
        "id": "X_EbGazkyGn5"
      },
      "id": "X_EbGazkyGn5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the text files from kaggle**"
      ],
      "metadata": {
        "id": "g3fMtNW6yGQv"
      },
      "id": "g3fMtNW6yGQv"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cy4swWHKLJM",
        "outputId": "c69376cd-ac4b-4ff9-ac5d-5308ed1d60c8"
      },
      "id": "0cy4swWHKLJM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "DhL83iexKSKt"
      },
      "id": "DhL83iexKSKt",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "wYC2eoBZKgy8",
        "outputId": "2b5fdb50-c5b9-47ae-9994-5162fcaf86b9"
      },
      "id": "wYC2eoBZKgy8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-87f9aecc-98e8-4926-8b5b-84364a2c2ec9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-87f9aecc-98e8-4926-8b5b-84364a2c2ec9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ashishkumardas1025\",\"key\":\"14651784e9015469581d00fee636902a\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle1\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfgFbd_R0gHP",
        "outputId": "adc7587e-bba2-4d12-dd84-6c330c74d9f3"
      },
      "id": "gfgFbd_R0gHP",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle1’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download pariza/bbc-news-summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZdvEdpU0rCL",
        "outputId": "0eecc7d5-847e-4d7e-8849-6f9dac51699f"
      },
      "id": "YZdvEdpU0rCL",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading bbc-news-summary.zip to /content\n",
            "\r  0% 0.00/8.91M [00:00<?, ?B/s]\n",
            "\r100% 8.91M/8.91M [00:00<00:00, 109MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting folders - News Articles, Summaries**"
      ],
      "metadata": {
        "id": "xyNy8PTlMoRT"
      },
      "id": "xyNy8PTlMoRT"
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile \n",
        "  \n",
        "# specifying the name of the zip file\n",
        "file = \"bbc-news-summary.zip\"\n",
        "  \n",
        "# open the zip file in read mode\n",
        "with ZipFile(file, 'r') as zip: \n",
        "    # extract all files to another directory\n",
        "    zip.extractall()\n",
        "    zip.close()"
      ],
      "metadata": {
        "id": "3YwtOoi61Dtj"
      },
      "id": "3YwtOoi61Dtj",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZRMkyXW0gMi"
      },
      "id": "XZRMkyXW0gMi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6e427857",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6e427857",
        "outputId": "5760f8f3-7536-4229-d796-f66d3ee78974"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            contents  \\\n",
              "0  \\nQuarterly profits at US media giant TimeWarn...   \n",
              "1  \\nThe dollar has hit its highest level against...   \n",
              "2  \\nThe owners of embattled Russian oil giant Yu...   \n",
              "3  \\nBritish Airways has blamed high fuel prices ...   \n",
              "4  \\nShares in UK drinks and food firm Allied Dom...   \n",
              "\n",
              "                                           summaries  \n",
              "0  TimeWarner said fourth quarter sales rose 2% t...  \n",
              "1  The dollar has hit its highest level against t...  \n",
              "2  Yukos' owner Menatep Group says it will ask Ro...  \n",
              "3  Rod Eddington, BA's chief executive, said the ...  \n",
              "4  Pernod has reduced the debt it took on to fund...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3f62ef4-40bf-49f2-aa91-41f41b167f2c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>contents</th>\n",
              "      <th>summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\nQuarterly profits at US media giant TimeWarn...</td>\n",
              "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\nThe dollar has hit its highest level against...</td>\n",
              "      <td>The dollar has hit its highest level against t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\nThe owners of embattled Russian oil giant Yu...</td>\n",
              "      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\nBritish Airways has blamed high fuel prices ...</td>\n",
              "      <td>Rod Eddington, BA's chief executive, said the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\nShares in UK drinks and food firm Allied Dom...</td>\n",
              "      <td>Pernod has reduced the debt it took on to fund...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3f62ef4-40bf-49f2-aa91-41f41b167f2c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3f62ef4-40bf-49f2-aa91-41f41b167f2c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3f62ef4-40bf-49f2-aa91-41f41b167f2c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "## We make a Dataframe from Data available in text format \n",
        "\n",
        "dictionary={'contents':[],'summaries':[]}\n",
        "for i in range(1,511):\n",
        "    if i<10:\n",
        "        f=open(f'/content/BBC News Summary/News Articles/business/00{i}.txt','r')\n",
        "        g=open(f'/content/BBC News Summary/Summaries/business/00{i}.txt','r')\n",
        "\n",
        "    elif 9<i<100:\n",
        "        f=open(f'/content/BBC News Summary/News Articles/business/0{i}.txt','r')\n",
        "        g=open(f'/content/BBC News Summary/Summaries/business/0{i}.txt','r')\n",
        "\n",
        "    else:\n",
        "        f=open(f'/content/BBC News Summary/News Articles/business/{i}.txt','r')\n",
        "        g=open(f'/content/BBC News Summary/Summaries/business/{i}.txt','r')\n",
        "    f.readline()           ## Ignore the heading\n",
        "    content,summary=f.read(),g.read()\n",
        "    dictionary['contents'].append(content)\n",
        "    dictionary['summaries'].append(summary)\n",
        "    f.close()\n",
        "    g.close()\n",
        "        \n",
        "import pandas as pd\n",
        "dataframe=pd.DataFrame(dictionary)\n",
        "dataframe.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install syllables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78QL3FQJj2jR",
        "outputId": "382f1e7e-019d-4aca-dc51-4de5dc8cf50d"
      },
      "id": "78QL3FQJj2jR",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting syllables\n",
            "  Downloading syllables-1.0.6-py3-none-any.whl (15 kB)\n",
            "Collecting cmudict<2.0.0,>=1.0.11\n",
            "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.3/939.3 KB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /usr/local/lib/python3.8/dist-packages (from cmudict<2.0.0,>=1.0.11->syllables) (5.10.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.11.0)\n",
            "Installing collected packages: importlib-metadata, cmudict, syllables\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 syllables-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "from textblob import TextBlob, Word\n",
        "import nltk                  \n",
        "import ssl \n",
        "import syllables"
      ],
      "metadata": {
        "id": "bCLOUjnC6tmg"
      },
      "id": "bCLOUjnC6tmg",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The most important problem at this point is Cleaning The Data\n",
        "\n",
        "1.   Convert to lowercase\n",
        "2.   Remove Unicode characters\n",
        "3.   Stop words removal **\n",
        "4.   Stemming/Lemmatization"
      ],
      "metadata": {
        "id": "sZciasTL2Wqg"
      },
      "id": "sZciasTL2Wqg"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0aac65c8",
      "metadata": {
        "id": "0aac65c8"
      },
      "outputs": [],
      "source": [
        "## STOPWORDS in Spacy \n",
        "\n",
        "\n",
        "stop_words=['those', 'on', 'own', '’ve', 'yourselves', 'around', 'between', 'four', 'been', 'alone', 'off', 'am', \n",
        "            'then', 'other', 'can', 'regarding', 'hereafter', 'front', 'too', 'used', 'wherein', '‘ll', 'doing', \n",
        "            'everything', 'up', 'onto', 'never', 'either', 'how', 'before', 'anyway', 'since', 'through', 'amount', \n",
        "            'now', 'he', 'was', 'have', 'into', 'because', 'therefore', 'they', 'even', 'whom', 'it', \n",
        "            'see', 'somewhere', 'thereupon', 'nothing', 'whereas', 'much', 'whenever', 'seem', 'until', 'whereby', \n",
        "            'at', 'also', 'some', 'last', 'than', 'get', 'already', 'our', 'once', 'will', 'noone', \"'m\", 'that', \n",
        "            'what', 'thus', 'no', 'myself', 'out', 'next', 'whatever', 'although', 'though', 'which', 'would', \n",
        "            'therein', 'nor', 'somehow', 'whereupon', 'besides', 'whoever', 'ourselves', 'few', 'did', 'without', \n",
        "            'third', 'anything', 'twelve', 'against', 'while', 'twenty', 'if', 'however', 'herself', 'when', 'may', \n",
        "            'ours', 'six', 'done', 'seems', 'else', 'call', 'perhaps', 'had', 'nevertheless', 'where', 'otherwise',\n",
        "            'still', 'within', 'its', 'for', 'together', 'elsewhere', 'throughout', 'of', 'others', 'show', '’s', \n",
        "            'anywhere', 'anyhow', 'as', 'are', 'the', 'hence', 'something', 'hereby', 'nowhere', 'latterly', 'say', \n",
        "            'does', 'neither', 'his', 'go', 'forty', 'put', 'their', 'by', 'namely', 'could', 'five', 'unless', \n",
        "            'itself', 'is', 'nine', 'whereafter', 'down', 'bottom', 'thereby', 'such', 'both', 'she', 'become', \n",
        "            'whole', 'who', 'yourself', 'every', 'thru', 'except', 'very', 'several', 'among', 'being', 'be', 'mine', \n",
        "            'further', 'n‘t', 'here', 'during', 'why', 'with', 'just', \"'s\", 'becomes', '’ll', 'about', 'a', 'using', \n",
        "            'seeming', \"'d\", \"'ll\", \"'re\", 'due', 'wherever', 'beforehand', 'fifty', 'becoming', 'might', 'amongst', \n",
        "            'my', 'empty', 'thence', 'thereafter', 'almost', 'least', 'someone', 'often', 'from', 'keep', 'him', \n",
        "            'or','‘m', 'top', 'her', 'nobody', 'sometime', 'across', '‘s', '’re', 'hundred', 'only', 'via', 'name', \n",
        "            'eight','three', 'back', 'to', 'all', 'became', 'move', 'me', 'we', 'formerly', 'so', 'i', 'whence', \n",
        "             'under', 'always', 'himself', 'in', 'herein', 'more', 'after', 'themselves', 'you', 'above', 'sixty', \n",
        "            'them', 'your', 'made', 'indeed', 'most', 'everywhere', 'fifteen', 'but', 'must', 'along', 'beside', \n",
        "            'hers', \n",
        "            'side', 'former', 'anyone', 'full', 'has', 'yours', 'whose', 'behind', 'please', 'ten', 'seemed', \n",
        "            'sometimes', 'should', 'over', 'take', 'each', 'same', 'rather', 'really', 'latter', 'and', 'ca', \n",
        "            'hereupon', 'part', 'per', 'eleven', 'ever', '‘re', 'enough', \"n't\", 'again', '‘d', 'us', 'yet', \n",
        "            'moreover', 'mostly', 'one', 'meanwhile', 'whither', 'there', 'toward', '’m', \"'ve\", '’d', 'give', 'do', \n",
        "            'an', 'quite', 'these', 'everyone', 'towards', 'this', 'cannot', 'afterwards', 'beyond', 'make', 'were', \n",
        "            'whether', 'well', 'another', 'below', 'first', 'upon', 'any', 'none', 'many', 'serious', 'various', \n",
        "            're', \n",
        "            'two', 'less', '‘ve']\n",
        "\n",
        "\n",
        "## Punctuations that exists in python\n",
        "\n",
        "import string\n",
        "punc=string.punctuation\n",
        "\n",
        "#import re\n",
        "#import nltk\n",
        "#from nltk.stem.snowball import SnowballStemmer  ## Snowball Stemmer is preferred over \n",
        "                                                ## Porter Stemmer & Lancaster Stemmer(aggresive one)\n",
        "#from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "#from textblob import TextBlob, Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "62527523",
      "metadata": {
        "id": "62527523",
        "outputId": "1425d526-7288-41f0-e2a9-a2164f8634b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#import nltk                  ## This particular block of code just helps us to download some\n",
        "#import ssl                   ## necessary tools which wasnt getting downloaded in simple manner\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a3d4ec13",
      "metadata": {
        "id": "a3d4ec13"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text=text.strip()    ## stripping white spaces at the end and front\n",
        "    text=text.lower()    ## Lowering every character\n",
        "    text=text.encode('ascii', 'ignore').decode()   ## Removing Unicode Characters\n",
        "    \n",
        "    ## Let's remove all the stopwords from the text(sentence)\n",
        "    text_list=text.split()   ## Double or More spaces in texts will be handled at this point\n",
        "    stopwords_removed_list=[]\n",
        "    '''for i in text_list:\n",
        "        if i not in stop_words:\n",
        "            temp_var=''\n",
        "            for j in i:\n",
        "                if j not in punc:\n",
        "                    temp_var+=str(j)\n",
        "            if temp_var not in stop_words:    \n",
        "                stopwords_removed_list.append(temp_var)\n",
        "    text=' '.join(stopwords_removed_list)  ## Sentence after stop words removal and punctuation removal\n",
        "\n",
        "    \n",
        "    \n",
        "    ## Having Done with points 1,2,3 in Text cleaning steps we progress to do stemming & Lemmatization'''\n",
        "    \n",
        "    ''' stemmer= SnowballStemmer(\"english\", ignore_stopwords=True) ## stopwords are already removed so \n",
        "                                                                   ##second parameter\n",
        "                                                                   ## may not have any added value\n",
        "\n",
        "    word_list=text.split()        ## Basic method to tokenize a sentence\n",
        "    \n",
        "    final_word_list=[snowBallStemmer.stem(word) for word in word_list]  ## Stemming every word\n",
        "    \n",
        "    text=' '.join(final_word_list) '''   ## Will use this part of code if needed \n",
        "    \n",
        "    ## It seems that lemmatization is better way to normalize texts\n",
        "    \n",
        "    word_list=text.split()\n",
        "    final=[]\n",
        "    for word in word_list:\n",
        "        w=lemmatizer.lemmatize(word)\n",
        "        final.append(w)\n",
        "    text=' '.join(final)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"answers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8N5kp0hc4Xzy",
        "outputId": "3ab4d9d8-0cc4-4b01-8e13-275e5b556973"
      },
      "id": "8N5kp0hc4Xzy",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'answer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del zip"
      ],
      "metadata": {
        "id": "1LClPhSv6NhQ"
      },
      "id": "1LClPhSv6NhQ",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ac7e6a0f",
      "metadata": {
        "id": "ac7e6a0f",
        "outputId": "abee4bc8-8703-44eb-95b9-3374e751ea02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Contents  \\\n",
              "0  quarterly profit at u medium giant timewarner ...   \n",
              "1  the dollar ha hit it highest level against the...   \n",
              "2  the owner of embattled russian oil giant yukos...   \n",
              "3  british airway ha blamed high fuel price for a...   \n",
              "4  share in uk drink and food firm allied domecq ...   \n",
              "\n",
              "                                           Summaries  \n",
              "0  TimeWarner said fourth quarter sales rose 2% t...  \n",
              "1  The dollar has hit its highest level against t...  \n",
              "2  Yukos' owner Menatep Group says it will ask Ro...  \n",
              "3  Rod Eddington, BA's chief executive, said the ...  \n",
              "4  Pernod has reduced the debt it took on to fund...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-838d831b-6766-46a6-9d2f-e9ab5a6c9bc7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Contents</th>\n",
              "      <th>Summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>quarterly profit at u medium giant timewarner ...</td>\n",
              "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the dollar ha hit it highest level against the...</td>\n",
              "      <td>The dollar has hit its highest level against t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the owner of embattled russian oil giant yukos...</td>\n",
              "      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>british airway ha blamed high fuel price for a...</td>\n",
              "      <td>Rod Eddington, BA's chief executive, said the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>share in uk drink and food firm allied domecq ...</td>\n",
              "      <td>Pernod has reduced the debt it took on to fund...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-838d831b-6766-46a6-9d2f-e9ab5a6c9bc7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-838d831b-6766-46a6-9d2f-e9ab5a6c9bc7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-838d831b-6766-46a6-9d2f-e9ab5a6c9bc7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "## Recreating Dataframe with cleaned entries and after normalizing texts\n",
        "\n",
        "content_list=dataframe['contents'].to_list()\n",
        "summaries_list=dataframe['summaries'].to_list()\n",
        "my_dict={'Contents':[],'Summaries':[]}\n",
        "for i,j in zip(content_list,summaries_list):\n",
        "    my_dict['Contents'],my_dict['Summaries']=my_dict['Contents']+[clean_text(i)],my_dict['Summaries']+[j]\n",
        "\n",
        "DF=pd.DataFrame(my_dict)\n",
        "DF.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## The models takes two parameters min_length and max_length\n",
        "\n",
        "summ_dict=dict()\n",
        "summary_list=DF['Summaries'].to_list()\n",
        "summ_list=[len(x) for x in summary_list]\n",
        "max_length=(sum(summ_list)//len(summ_list))  ## Average summary length stored in parameter max_length\n",
        "min_length=min(summ_list)\n",
        "print(min_length,max_length)\n",
        "\n",
        "content_list=DF['Contents'].to_list()\n",
        "cont_list=[len(i) for i in content_list]\n",
        "print(int(0.333333333*(sum(cont_list)//len(cont_list))))\n",
        "\n",
        "## We use the fact that length of summary of an article  will lie between\n",
        "## minimum length of all summaries given in our dataset and one-third of average length of the Contents \n",
        "\n",
        "## IN OTHER WORDS LENGTH OF SUMMARY OF AN ARTICLE LIES IN [243,638]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbcGoJvQtRny",
        "outputId": "4798d9e7-89df-44bc-9c93-efd3a10dda0b"
      },
      "id": "dbcGoJvQtRny",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "243 865\n",
            "638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "kcQRRF8TkW1G"
      },
      "id": "kcQRRF8TkW1G",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_metric\n",
        "!pip install nltk rouge_score\n",
        "\n",
        "from rouge_score import rouge_scorer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DR_lbk-OOecl",
        "outputId": "d6c400d3-76e3-4572-f9cf-05e4bb985d74"
      },
      "id": "DR_lbk-OOecl",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.21.6)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.15.0)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=1660fadaf4c6737b6c8cbc356000452b0f43e0cbe570b5b65d552a88052e9292\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pegasus - 568M parameters - pretrained on C4(750GB) and HugeNews(3.8TB) - GSG - principal gap selection**"
      ],
      "metadata": {
        "id": "deBCVje0PG9R"
      },
      "id": "deBCVje0PG9R"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q5Cer7fdPEaS"
      },
      "id": "Q5Cer7fdPEaS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/pegasus-xsum\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "## We use a Loop over the dataframe to summarize every content\n",
        "\n",
        "summarized_contents=[]\n",
        "frac = 0.2\n",
        "\n",
        "for i in range(10):\n",
        "  article=DF['Contents'][i]\n",
        "  max_len = frac*(len(article.split()))\n",
        "  inputs = tokenizer(article, truncation = True,padding='longest', return_tensors=\"pt\").to(device)\n",
        "  inp_dec = model.generate(**inputs, max_length = max_len)\n",
        "  summarized = tokenizer.batch_decode(inp_dec, skip_special_tokens=True)\n",
        "  summarized_contents.append(summarized)"
      ],
      "metadata": {
        "id": "IN7UmzO6lXVm"
      },
      "id": "IN7UmzO6lXVm",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "PyhsuPFqd92s"
      },
      "id": "PyhsuPFqd92s",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization = pipeline(\"summarization\")\n",
        "summarized_contents_=[]\n",
        "for i in range(10):\n",
        "  summary_text = summarization(DF['Contents'][i])[0]\n",
        "  summarized_contents_.append(summary_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euI261BLc4O0",
        "outputId": "27295c3a-f56b-4503-a42e-2ff92b589ac8"
      },
      "id": "euI261BLc4O0",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_contents_[1]['summary_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "d8_KHfhZP5Cf",
        "outputId": "e88cef1e-6107-4c74-fce5-ca516e777517"
      },
      "id": "d8_KHfhZP5Cf",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' dollar hits highest level against euro in almost three month . Federal reserve head says trade deficit is set to stabilise . Greenback tumbled on the back of worse-than-expected u job data . Concern about the deficit concern about china do, however, remain .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "DydQc_iTceun"
      },
      "id": "DydQc_iTceun",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## We have calculated the metric for the first 5 rows of our dataset\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
        "precision=[]\n",
        "precision_=[]\n",
        "recall=[]\n",
        "recall_=[]\n",
        "f1=[]\n",
        "f1_=[]\n",
        "for i in range(10):\n",
        "  score = scorer.score(summarized_contents[i][0],DF['Summaries'][0])\n",
        "  precision.append(score['rouge2'].precision)\n",
        "  recall.append(score['rouge2'].recall)\n",
        "  f1.append(score['rouge2'].fmeasure)\n",
        "\n",
        "for i in range(10):\n",
        "  score = scorer.score(summarized_contents_[i]['summary_text'],DF['Summaries'][0])\n",
        "  precision_.append(score['rouge2'].precision)\n",
        "  recall_.append(score['rouge2'].recall)\n",
        "  f1_.append(score['rouge2'].fmeasure)\n",
        "\n",
        "\n",
        "metric_df = pd.DataFrame({'precision' : [np.mean(precision), np.mean(precision_)],\n",
        "                          'recall' : [np.mean(recall), np.mean(recall_)],\n",
        "                          'f1' : [np.mean(f1), np.mean(f1_)]})"
      ],
      "metadata": {
        "id": "Seiu8SI1Ph9J"
      },
      "id": "Seiu8SI1Ph9J",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "DK7fLO3fb0b7",
        "outputId": "bd7b9308-1241-4607-fb62-aa9e53bb7d20"
      },
      "id": "DK7fLO3fb0b7",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   precision    recall        f1\n",
              "0   0.005263  0.049848  0.009466\n",
              "1   0.028947  0.112679  0.045972"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d81a958-68aa-4d78-a359-ae48b9a21a1f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.005263</td>\n",
              "      <td>0.049848</td>\n",
              "      <td>0.009466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.028947</td>\n",
              "      <td>0.112679</td>\n",
              "      <td>0.045972</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d81a958-68aa-4d78-a359-ae48b9a21a1f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d81a958-68aa-4d78-a359-ae48b9a21a1f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d81a958-68aa-4d78-a359-ae48b9a21a1f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oync87Z7t35X",
        "outputId": "b89d0f2c-2d8f-4947-b987-2e5e7085d39b"
      },
      "id": "Oync87Z7t35X",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  114,  4911,   657,  1844, 29752,   109,   531,   131,   116,  1715,\n",
              "          9446,   301,   113,  3693,   164,   109,  1298,   113,  6003,  8886,\n",
              "           174,  6723,   165,   141,   142,  3455,  1462,   107,   109,  1806,\n",
              "           118, 92107, 10970,   143, 31788, 10970,   158,   233,  3252,   141,\n",
              "           109, 64545, 41604,  2633,   115,  8094,   233, 21445,  7752,   115,\n",
              "           114, 16392,  1057,   107,   109,  1462,   115, 63409,   374,   120,\n",
              "           109,   437,   256,   146,   129,  1457,   365,  2043,  1857,   121,\n",
              "         24884,  5009, 37668,  2564,   107,   790,   109,  6462,   195, 20913,\n",
              "          8330,   456,   108,   110, 63640,   920, 10152, 25856,  9446,   108,\n",
              "           110, 15548, 41062,  9446,   108, 19683, 62354,   456,   111,  2799,\n",
              "           111, 75606,   661,   107,   115,   126,   437,   108,   109,   657,\n",
              "          4620,  9446,  1419, 25735, 21628,   476,   112,   815,  6030,   108,\n",
              "          4635, 10686,   122,  1546,   121, 22979,  4322,  2429,  4515,   108,\n",
              "         26883,   160,   109,  5410,   113,  6003,   111,  8631,   473,   112,\n",
              "           109, 10137,   107, 19967,   869,   109, 15098,  1419,   112, 16553,\n",
              "         92107, 10970,   115,  3508, 12837,   204,   109,   555,  1149,   232,\n",
              "           111, 14281, 17179,  2613,   124,   834,   153,   379,   107,   155,\n",
              "           109,  1462,   113,  3455,   118,   109,  2607,   113,   110, 71003,\n",
              "          8258,   120,   109,  4911,   657,   256,   146, 17984,   109,  1419,\n",
              "           365,  4024,  4188,   164,   112, 30325, 59803, 38047,   113,   260,\n",
              "           107,   109,  9446,   301, 10262,   120,   157, 17695, 68929,   112,\n",
              "          2226,  6003,   111, 43655,   109,   481,   107,   157,   163,   416,\n",
              "           157,   133,   506,  1676,   223,   113,   109,   657,   131,   116,\n",
              "          1806,   115,   114, 11023,  7860,  1717, 10970,  5183,  2455,   122,\n",
              "          7052,   449,   115, 15326,   537,   113,  9446,   301,  2127,   902,\n",
              "           244,   109,  7338,   108,   122, 20913,  8330,  4220,  8163,   111,\n",
              "           920, 10152, 25856,  1986,  1830,   113,   110, 45364,   107,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarized_contents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msONua8TLuN7",
        "outputId": "08920db5-37ab-4ea6-e2da-13220563e49e"
      },
      "id": "msONua8TLuN7",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Timewarner has reported a sharp rise in fourth quarter profit, helped by a one-off gain from its stake in google.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DF['Contents'][0]"
      ],
      "metadata": {
        "id": "kNkmuXpOIOlM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "6c1681b3-ed1a-4ab1-9f46-a7f721cdbcf9"
      },
      "id": "kNkmuXpOIOlM",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'quarterly profit at u medium giant timewarner jumped 76% to $1.13bn (600m) for the three month to december, from $639m year-earlier. the firm, which is now one of the biggest investor in google, benefited from sale of high-speed internet connection and higher advert sales. timewarner said fourth quarter sale rose 2% to $11.1bn from $10.9bn. it profit were buoyed by one-off gain which offset a profit dip at warner bros, and le user for aol. time warner said on friday that it now owns 8% of search-engine google. but it own internet business, aol, had ha mixed fortunes. it lost 464,000 subscriber in the fourth quarter profit were lower than in the preceding three quarters. however, the company said aol\\'s underlying profit before exceptional item rose 8% on the back of stronger internet advertising revenues. it hope to increase subscriber by offering the online service free to timewarner internet customer and will try to sign up aol\\'s existing customer for high-speed broadband. timewarner also ha to restate 2000 and 2003 result following a probe by the u security exchange commission (sec), which is close to concluding. time warner\\'s fourth quarter profit were slightly better than analysts\\' expectations. but it film division saw profit slump 27% to $284m, helped by box-office flop alexander and catwoman, a sharp contrast to year-earlier, when the third and final film in the lord of the ring trilogy boosted results. for the full-year, timewarner posted a profit of $3.36bn, up 27% from it 2003 performance, while revenue grew 6.4% to $42.09bn. \"our financial performance wa strong, meeting or exceeding all of our full-year objective and greatly enhancing our flexibility,\" chairman and chief executive richard parson said. for 2005, timewarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins. timewarner is to restate it account a part of effort to resolve an inquiry into aol by u market regulators. it ha already offered to pay $300m to settle charges, in a deal that is under review by the sec. the company said it wa unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. it intends to adjust the way it account for a deal with german music publisher bertelsmann\\'s purchase of a stake in aol europe, which it had reported a advertising revenue. it will now book the sale of it stake in aol europe a a loss on the value of that stake.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Steps:** We will use newspaper library to webscrape website's article (allowing webscrapping) and summarize it i.e., URL based Summarization and will deploy the pegasus model using Streamlit platform."
      ],
      "metadata": {
        "id": "42SlNjnqTOKk"
      },
      "id": "42SlNjnqTOKk"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.3\n",
        "!pip install streamlit -q\n",
        "!pip install transformers\n",
        "\n",
        "!pip install newspaper3k\n",
        "!pip install sentencepiece\n",
        "!pip install beautifulsoup4\n",
        "!pip install google"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_9zJUrrkJBh",
        "outputId": "99b7b69f-01da-4554-e93f-e39d6ea64774"
      },
      "id": "m_9zJUrrkJBh",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.3\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (4.9.2)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (2.25.1)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (6.0)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (3.7)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2.1->newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->newspaper3k) (1.26.14)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.9.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13552 sha256=fcce65d0c6fe6522a0840c7c7df48e7fb99d379bce68af97781d192c54af0b10\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/74/83/8fac1c8d9c648cfabebbbffe97a889f6624817f3aa0bbe6c09\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3354 sha256=18b8df16773d24c05dd77a6a1fc18c8be5cc94a0b5a2e8a86884f8258e6203ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/09/68/a9f15498ac02c23dde29f18745bc6a6f574ba4ab41861a3575\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398405 sha256=8bc3d4da05a10303966fbcf82e9f4e45da6ff1ed71c52728f5818294586c114f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/7e/0c/54f3b0f5164278677899f2db08f2b07943ce2d024a3c862afb\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=7653d0107cc57c3b5a81232b19ad382a9a68cf2494fe9b63f3e200f5637173f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.8/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from google) (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6SY7_tak4Uz"
      },
      "id": "g6SY7_tak4Uz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc34084-1244-44ad-bd33-92186ca1d1bf",
        "id": "kWB6dXDwtOXf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing summarize.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile summarize.py\n",
        "import streamlit as st\n",
        "from newspaper import Article\n",
        "import nltk\n",
        "from math import floor\n",
        "from PIL import Image\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "st.image(\"https://eecsnews.engin.umich.edu/wp-content/uploads/sites/2/2021/03/summarization-featured.jpg\")\n",
        "#st.text_input(\"FINANCIAL TEXT SUMMARIZATION\", kwargs=None, placeholder=None, disabled=True)\n",
        "\n",
        "url = st.text_input(\n",
        "        \"input the URL👇\")\n",
        "if url:  \n",
        "  article = Article(url)\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  #article.text\n",
        "  def read_text(path):\n",
        "    with open(path,'r') as f:\n",
        "      lines=f.readlines()\n",
        "    return lines[0]\n",
        "  model_name = \"google/pegasus-xsum\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "  article=article.text\n",
        "  print(article)\n",
        "  max_len = floor(0.2*(len(article.split())))\n",
        "  inputs = tokenizer(article, truncation = True,padding='longest', return_tensors=\"pt\").to(device)\n",
        "  inp_dec = model.generate(**inputs, max_length = max_len)\n",
        "  summarized = tokenizer.batch_decode(inp_dec, skip_special_tokens=True)\n",
        "  summarized = \" \".join(summarized)\n",
        "  st.write(summarized)\n"
      ],
      "id": "kWB6dXDwtOXf"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgbBlSF3v-nH",
        "outputId": "f3cebb0d-2ac5-4fee-ee15-40e0d1bfcce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting summarize.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile summarize.py\n",
        "import streamlit as st\n",
        "from newspaper import Article\n",
        "import nltk\n",
        "from math import floor\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "import string\n",
        "import syllables\n",
        "import math\n",
        "import random\n",
        "punc=string.punctuation\n",
        "\n",
        "def read_text(path):\n",
        "  with open(path,'r') as f:\n",
        "    lines=f.readlines()\n",
        "  return lines[0]\n",
        "\n",
        "def mod_sen(sentence):\n",
        "  modi_sen=''\n",
        "  for i in sentence:\n",
        "    if i in punc:\n",
        "      modi_sen+='.'\n",
        "    else:\n",
        "      modi_sen+=i\n",
        "  return modi_sen\n",
        "\n",
        "def flesch_kincaid(text):\n",
        "    text=mod_sen(text)\n",
        "    total_sentences,total_words=len(text.split('.'))-1,(len(text.split()))+text.count('.')-1\n",
        "    sentences=text.split('.')\n",
        "    syl_count=0\n",
        "    for i in sentences:\n",
        "      l=i.split()\n",
        "      for j in l:\n",
        "        syl_count+=syllables.estimate(j)\n",
        "    print(syl_count)\n",
        "    reading_ease=(210.835-(7.015*(total_words/total_sentences))-(90.6*(syl_count/total_words)))\n",
        "    reading_grade=((0.39*(total_words/total_sentences))+(11.8*(syl_count/total_words))-15.59)\n",
        "    if reading_ease>0:\n",
        "      return math.floor(reading_ease)\n",
        "    else:\n",
        "      epsilon=(random.random())/100\n",
        "      return epsilon\n",
        "\n",
        "def fog_index(text):\n",
        "  text=mod_sen(text)\n",
        "  total_sentences,total_words=len(text.split('.'))-1,(len(text.split()))+text.count('.')-1\n",
        "  sentences=text.split('.')\n",
        "  complexity=0\n",
        "  for i in sentences:\n",
        "    l=i.split()\n",
        "    for j in l:\n",
        "      if syllables.estimate(j)>2:\n",
        "        complexity+=1\n",
        "  return (0.7*((total_words/total_sentences)+100*(complexity/total_words)))\n",
        "\n",
        "st.image(\"https://eecsnews.engin.umich.edu/wp-content/uploads/sites/2/2021/03/summarization-featured.jpg\")\n",
        "url = st.text_input(\n",
        "        \"input url\")\n",
        "if url:  \n",
        "  article = Article(url)\n",
        "  article.download()\n",
        "  article.parse()\n",
        "\n",
        "\n",
        "  model_name = \"google/pegasus-xsum\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "  article=article.text\n",
        "  t=mod_sen(article)\n",
        "  l=t.split()\n",
        "  o_standard_reading_time=len(l)/80\n",
        "  max_len = floor(0.2*(len(article.split())))\n",
        "  inputs = tokenizer(article, truncation = True,padding='longest', return_tensors=\"pt\").to(device)\n",
        "  inp_dec = model.generate(**inputs, max_length = max_len)\n",
        "  summarized = tokenizer.batch_decode(inp_dec, skip_special_tokens=True)\n",
        "  summarized = \" \".join(summarized)\n",
        "  s_standard_reading_time=len(summarized.split())/80\n",
        "  st.write(summarized)\n",
        "\n",
        "  st.write(f'The Readability score of the original article is {flesch_kincaid(article)}')\n",
        "  st.write(f'The Reading time of the original text is {o_standard_reading_time} mins')\n",
        "  st.write(f'The Readability score of the summarized text is {flesch_kincaid(summarized)}')\n",
        "  st.write(f'The Reading time of the summarized text is {s_standard_reading_time} mins')"
      ],
      "id": "lgbBlSF3v-nH"
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run summarize.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZBO-Nn3lPU1",
        "outputId": "570747bb-1a3e-4740-a18d-eb1f86cb114e"
      },
      "id": "IZBO-Nn3lPU1",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[########..........] / extract:openurl: sill extract string-width@^4.2.0 extrac\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.861s\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.85.254.224:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://late-symbols-carry-34-85-254-224.loca.lt\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "447\n",
            "47\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XFj_EnlElSVM"
      },
      "id": "XFj_EnlElSVM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3910jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}